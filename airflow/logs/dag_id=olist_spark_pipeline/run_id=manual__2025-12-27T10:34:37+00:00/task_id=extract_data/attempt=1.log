{"timestamp":"2025-12-27T10:34:40.497957Z","level":"info","event":"DAG bundles loaded: dags-folder, example_dags","logger":"airflow.dag_processing.bundles.manager.DagBundlesManager","filename":"manager.py","lineno":179}
{"timestamp":"2025-12-27T10:34:40.504815Z","level":"info","event":"Filling up the DagBag from /opt/airflow/dags/ecom_pipeline.py","logger":"airflow.models.dagbag.DagBag","filename":"dagbag.py","lineno":593}
{"timestamp":"2025-12-27T10:34:40.652893Z","level":"info","event":"Task instance is in running state","logger":"task.stdout"}
{"timestamp":"2025-12-27T10:34:40.653962Z","level":"info","event":" Previous state of the Task instance: TaskInstanceState.QUEUED","logger":"task.stdout"}
{"timestamp":"2025-12-27T10:34:40.654091Z","level":"info","event":"Current task name:extract_data","logger":"task.stdout"}
{"timestamp":"2025-12-27T10:34:40.654164Z","level":"info","event":"Dag name:olist_spark_pipeline","logger":"task.stdout"}
{"timestamp":"2025-12-27T10:34:40.653542Z","level":"info","event":"Tmp dir root location: /tmp","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook","filename":"subprocess.py","lineno":78}
{"timestamp":"2025-12-27T10:34:40.660192Z","level":"info","event":"Running command: ['/usr/bin/bash', '-c', '\\n        spark-submit opt/src/extract.py\\n        ']","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook","filename":"subprocess.py","lineno":80}
{"timestamp":"2025-12-27T10:34:40.662284Z","level":"info","event":"Output:","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook","filename":"subprocess.py","lineno":92}
{"timestamp":"2025-12-27T10:34:40.693609Z","level":"info","event":"/opt/spark/bin/spark-class: line 71: /usr/lib/jvm/java-17-openjdk-amd64/bin/java: No such file or directory","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook","filename":"subprocess.py","lineno":99}
{"timestamp":"2025-12-27T10:34:40.694309Z","level":"info","event":"/opt/spark/bin/spark-class: line 97: CMD: bad array subscript","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook","filename":"subprocess.py","lineno":99}
{"timestamp":"2025-12-27T10:34:40.696869Z","level":"info","event":"Command exited with return code 1","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook","filename":"subprocess.py","lineno":103}
{"timestamp":"2025-12-27T10:34:40.697604Z","level":"error","event":"Task failed with exception","logger":"task","filename":"task_runner.py","lineno":986,"error_detail":[{"exc_type":"AirflowException","exc_value":"Bash command failed. The command returned a non-zero exit code 1.","exc_notes":[],"syntax_error":null,"is_cause":false,"frames":[{"filename":"/home/airflow/.local/lib/python3.12/site-packages/airflow/sdk/execution_time/task_runner.py","lineno":934,"name":"run"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/airflow/sdk/execution_time/task_runner.py","lineno":1320,"name":"_execute_task"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/airflow/sdk/bases/operator.py","lineno":417,"name":"wrapper"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/airflow/providers/standard/operators/bash.py","lineno":224,"name":"execute"}],"is_group":false,"exceptions":[]}]}
{"timestamp":"2025-12-27T10:34:40.728057Z","level":"info","event":"Task instance in failure state","logger":"task.stdout"}
{"timestamp":"2025-12-27T10:34:40.733829Z","level":"info","event":"Task start","logger":"task.stdout"}
{"timestamp":"2025-12-27T10:34:40.734062Z","level":"info","event":"Task:<Task(BashOperator): extract_data>","logger":"task.stdout"}
{"timestamp":"2025-12-27T10:34:40.734135Z","level":"info","event":"Failure caused by Bash command failed. The command returned a non-zero exit code 1.","logger":"task.stdout"}
