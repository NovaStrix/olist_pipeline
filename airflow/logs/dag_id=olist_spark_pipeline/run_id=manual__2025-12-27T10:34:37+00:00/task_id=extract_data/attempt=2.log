{"timestamp":"2025-12-27T10:39:42.538004Z","level":"info","event":"DAG bundles loaded: dags-folder, example_dags","logger":"airflow.dag_processing.bundles.manager.DagBundlesManager","filename":"manager.py","lineno":179}
{"timestamp":"2025-12-27T10:39:42.539926Z","level":"info","event":"Filling up the DagBag from /opt/airflow/dags/ecom_pipeline.py","logger":"airflow.models.dagbag.DagBag","filename":"dagbag.py","lineno":593}
{"timestamp":"2025-12-27T10:39:42.592927Z","level":"info","event":"Task instance is in running state","logger":"task.stdout"}
{"timestamp":"2025-12-27T10:39:42.593692Z","level":"info","event":" Previous state of the Task instance: TaskInstanceState.QUEUED","logger":"task.stdout"}
{"timestamp":"2025-12-27T10:39:42.595981Z","level":"info","event":"Current task name:extract_data","logger":"task.stdout"}
{"timestamp":"2025-12-27T10:39:42.596153Z","level":"info","event":"Dag name:olist_spark_pipeline","logger":"task.stdout"}
{"timestamp":"2025-12-27T10:39:42.592521Z","level":"info","event":"Tmp dir root location: /tmp","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook","filename":"subprocess.py","lineno":78}
{"timestamp":"2025-12-27T10:39:42.595044Z","level":"info","event":"Running command: ['/usr/bin/bash', '-c', '\\n        spark-submit opt/src/extract.py\\n        ']","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook","filename":"subprocess.py","lineno":80}
{"timestamp":"2025-12-27T10:39:42.596706Z","level":"info","event":"Output:","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook","filename":"subprocess.py","lineno":92}
{"timestamp":"2025-12-27T10:39:42.616272Z","level":"info","event":"/opt/spark/bin/spark-class: line 71: /usr/lib/jvm/java-17-openjdk-amd64/bin/java: No such file or directory","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook","filename":"subprocess.py","lineno":99}
{"timestamp":"2025-12-27T10:39:42.616794Z","level":"info","event":"/opt/spark/bin/spark-class: line 97: CMD: bad array subscript","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook","filename":"subprocess.py","lineno":99}
{"timestamp":"2025-12-27T10:39:42.619331Z","level":"info","event":"Command exited with return code 1","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook","filename":"subprocess.py","lineno":103}
{"timestamp":"2025-12-27T10:39:42.619911Z","level":"error","event":"Task failed with exception","logger":"task","filename":"task_runner.py","lineno":986,"error_detail":[{"exc_type":"AirflowException","exc_value":"Bash command failed. The command returned a non-zero exit code 1.","exc_notes":[],"syntax_error":null,"is_cause":false,"frames":[{"filename":"/home/airflow/.local/lib/python3.12/site-packages/airflow/sdk/execution_time/task_runner.py","lineno":934,"name":"run"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/airflow/sdk/execution_time/task_runner.py","lineno":1320,"name":"_execute_task"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/airflow/sdk/bases/operator.py","lineno":417,"name":"wrapper"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/airflow/providers/standard/operators/bash.py","lineno":224,"name":"execute"}],"is_group":false,"exceptions":[]}]}
{"timestamp":"2025-12-27T10:39:42.638805Z","level":"info","event":"Task instance in failure state","logger":"task.stdout"}
{"timestamp":"2025-12-27T10:39:42.641791Z","level":"info","event":"Task start","logger":"task.stdout"}
{"timestamp":"2025-12-27T10:39:42.642980Z","level":"info","event":"Task:<Task(BashOperator): extract_data>","logger":"task.stdout"}
{"timestamp":"2025-12-27T10:39:42.644950Z","level":"info","event":"Failure caused by Bash command failed. The command returned a non-zero exit code 1.","logger":"task.stdout"}
